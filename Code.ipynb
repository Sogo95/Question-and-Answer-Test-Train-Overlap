{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sogo95/Question-and-Answer-Test-Train-Overlap/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8y2z5EpIUJy",
        "outputId": "968ba3bd-6cfe-41da-897d-59c12315cb12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=6778fdb8379f9e4c794cf45779489a8317fafbb4fb1448281207133683949840\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "pip install wget\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Téléchargement des données"
      ],
      "metadata": {
        "id": "-baJNzKzskiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2020-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\"\"\"Download data for calculating question overlap\"\"\"\n",
        "import wget\n",
        "\n",
        "\n",
        "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
        "#DIRNAME = os.path.dirname(os.path.abspath(__file__))\n",
        "#DATA_DIR = os.path.join(DIRNAME, 'data')\n",
        "\n",
        "TEST_SETS_TO_DOWNLOAD = [\n",
        "    ('https://dl.fbaipublicfiles.com/qaoverlap/data/nq-test.qa.csv','nq-test.qa.csv'),\n",
        "    ('https://dl.fbaipublicfiles.com/qaoverlap/data/triviaqa-test.qa.csv', 'triviaqa-test.qa.csv'),\n",
        "    ('https://dl.fbaipublicfiles.com/qaoverlap/data/webquestions-test.qa.csv', 'webquestions-test.qa.csv'),\n",
        "]\n",
        "ANNOTATIONS_TO_DOWNLOAD = [\n",
        "    ('https://dl.fbaipublicfiles.com/qaoverlap/data/nq-annotations.jsonl','nq-annotations.jsonl'),\n",
        "    ('https://dl.fbaipublicfiles.com/qaoverlap/data/triviaqa-annotations.jsonl', 'triviaqa-annotations.jsonl'),\n",
        "    ('https://dl.fbaipublicfiles.com/qaoverlap/data/webquestions-annotations.jsonl','webquestions-annotations.jsonl')\n",
        "]\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "for link, dest in TEST_SETS_TO_DOWNLOAD:\n",
        "    wget.download(link, os.path.join(DATA_DIR, dest))\n",
        "\n",
        "for link, dest in ANNOTATIONS_TO_DOWNLOAD:\n",
        "    wget.download(link, os.path.join(DATA_DIR, dest))\n"
      ],
      "metadata": {
        "id": "MlHNCcjjdcp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation des datasets"
      ],
      "metadata": {
        "id": "8xVQltYssuug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import argparse\n",
        "import ast\n",
        "\n",
        "ANNOTATIONS = [\n",
        "    'total',\n",
        "    'question_overlap',\n",
        "    'no_question_overlap',\n",
        "    'answer_overlap',\n",
        "    'no_answer_overlap',\n",
        "    'answer_overlap_only'\n",
        "]\n",
        "\n",
        "DIRNAME = os.getcwd()\n",
        "DATA_DIR = os.path.join(DIRNAME, 'data')\n",
        "REFERENCE_PATHS = {\n",
        "    'triviaqa': os.path.join(DIRNAME, 'data/triviaqa-test.qa.csv'),\n",
        "    'nq': os.path.join(DIRNAME, 'data/nq-test.qa.csv'),\n",
        "    'webquestions': os.path.join(DIRNAME, 'data/webquestions-test.qa.csv'),\n",
        "}\n",
        "ANNOTATION_PATHS = {\n",
        "    'triviaqa': os.path.join(DIRNAME, 'data/triviaqa-annotations.jsonl'),\n",
        "    'nq': os.path.join(DIRNAME, 'data/nq-annotations.jsonl'),\n",
        "    'webquestions': os.path.join(DIRNAME, 'data/webquestions-annotations.jsonl'),\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        " # Handle list inputs\n",
        "    if isinstance(s, list):\n",
        "        s = ' '.join(s)  # Join list elements into a single string\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def read_references(fi, sep='\\t'):\n",
        "    def parse_pandas_answer(a_string):\n",
        "        try:\n",
        "            parsed_answers = ast.literal_eval(a_string) if a_string.startswith('[') else ast.literal_eval(a_string.replace('\"\"', '\"')[1:-1])\n",
        "        except:\n",
        "            parsed_answers = ast.literal_eval(a_string.replace('\"\"', '\"').replace('\"\"', '\"').replace('\"\"', '\"')[1:-1])\n",
        "        return parsed_answers\n",
        "\n",
        "\n",
        "    references = []\n",
        "    for i, line in enumerate(open(fi)):\n",
        "        q, answer_str = line.strip('\\n').split(sep)\n",
        "        refs = parse_pandas_answer(answer_str)\n",
        "        references.append({'references': refs, 'id': i})\n",
        "    return references\n",
        "\n",
        "\n",
        "def read_jsonl(path):\n",
        "    with open(path) as f:\n",
        "        return [json.loads(l) for l in f]\n",
        "\n",
        "\n",
        "def read_lines(path):\n",
        "    with open(path) as f:\n",
        "        return [l.strip() for l in f]\n",
        "\n",
        "\n",
        "def read_annotations(annotations_data_path):\n",
        "    return read_jsonl(annotations_data_path)\n",
        "\n",
        "\n",
        "def safe_parse_prediction(prediction_str):\n",
        "    \"\"\"Essaie d'évaluer la prédiction en utilisant ast.literal_eval. Renvoie la chaîne brute en cas d'échec.\"\"\"\n",
        "    try:\n",
        "        return ast.literal_eval(prediction_str)\n",
        "    except (SyntaxError, ValueError) as e:\n",
        "        print(f\"Erreur lors de l'évaluation de la prédiction : {prediction_str}\")\n",
        "        print(f\"Détails de l'erreur : {e}\")\n",
        "        return prediction_str  # Retourne la prédiction sous forme de chaîne brute si l'évaluation échoue\n",
        "\n",
        "\n",
        "def read_predictions(path):\n",
        "    if path.endswith('json') or path.endswith('.jsonl'):\n",
        "        return read_jsonl(path)\n",
        "    else:\n",
        "        return [{'id': i, 'prediction': safe_parse_prediction(pred.split('\\t')[1])} for i, pred in enumerate(read_lines(path))]\n",
        "\n",
        "\n",
        "def _get_scores(answers, refs, fn):\n",
        "    return [metric_max_over_ground_truths(fn, pred, rs) for pred, rs in zip(answers, refs)]\n",
        "\n",
        "\n",
        "def get_scores(predictions, references, annotations, annotation_labels=None):\n",
        "    predictions_map = {p['id']: p for p in predictions}\n",
        "    references_map = {r['id']: r for r in references}\n",
        "    annotations_map = {a['id']: a for a in annotations}\n",
        "    assert predictions_map.keys() == references_map.keys(), 'predictions file doesnt match the gold references file '\n",
        "    assert predictions_map.keys() == annotations_map.keys(), 'prediction file doesnt match the annotation file '\n",
        "    assert annotations_map.keys() == references_map.keys(), 'annotations file doesnt match the gold references file '\n",
        "\n",
        "    annotation_labels = ANNOTATIONS if annotation_labels is None else annotation_labels\n",
        "\n",
        "    results = {}\n",
        "    for annotation_label in annotation_labels:\n",
        "        annotation_ids = [annotation['id'] for annotation in annotations if annotation_label in annotation['labels']]\n",
        "        preds = [predictions_map[idd]['prediction'] for idd in annotation_ids]\n",
        "        refs = [references_map[idd]['references'] for idd in annotation_ids]\n",
        "        em = _get_scores(preds, refs, exact_match_score)\n",
        "        f = _get_scores(preds, refs, f1_score)\n",
        "        results[annotation_label] = {\n",
        "            'exact_match': 100 * sum(em) / len(em),\n",
        "            'f1_score': 100 * sum(f) / len(f),\n",
        "            'n_examples': len(annotation_ids),\n",
        "        }\n",
        "    return results\n",
        "\n",
        "\n",
        "def _print_score(label, results_dict):\n",
        "    print('-' * 50)\n",
        "    print('Label       :', label)\n",
        "    print('N examples  : ', results_dict['n_examples'])\n",
        "    print('Exact Match : ', results_dict['exact_match'])\n",
        "    print('F1 score    : ', results_dict['f1_score'])\n",
        "\n",
        "\n",
        "def _main(predictions_path, references_path, annotations_path):\n",
        "    predictions = read_predictions(predictions_path)\n",
        "    references = read_references(references_path)\n",
        "    annotations = read_annotations(annotations_path)\n",
        "    scores = get_scores(predictions, references, annotations)\n",
        "    for label in ANNOTATIONS:\n",
        "        _print_score(label, scores[label])\n",
        "\n",
        "\n",
        "def main(predictions_path, dataset_name):\n",
        "    references_path = REFERENCE_PATHS[dataset_name]\n",
        "    annotations_path = ANNOTATION_PATHS[dataset_name]\n",
        "    if not os.path.exists(references_path):\n",
        "        raise Exception('References expected at ' + references_path\n",
        "                        + ' not found, please download them using the download script (see readme)')\n",
        "    if not os.path.exists(annotations_path):\n",
        "        raise Exception('Annotations expected at ' + annotations_path\n",
        "                        + ' not found, please download them using the download script (see readme)')\n",
        "    _main(predictions_path, references_path, annotations_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    datasets = ['nq', 'triviaqa', 'webquestions']\n",
        "    for dataset_name in datasets:\n",
        "        predictions_path = os.path.join(DATA_DIR, f'{dataset_name}-test.qa.csv')\n",
        "        print(f'Évaluation pour le dataset: {dataset_name}')\n",
        "        main(predictions_path, dataset_name)\n",
        "        print()\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phYz6cWlqiZZ",
        "outputId": "d4f15de5-38da-44c3-f10f-b499f6034d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Évaluation pour le dataset: nq\n",
            "--------------------------------------------------\n",
            "Label       : total\n",
            "N examples  :  3610\n",
            "Exact Match :  57.8393351800554\n",
            "F1 score    :  85.74091619998404\n",
            "--------------------------------------------------\n",
            "Label       : question_overlap\n",
            "N examples  :  324\n",
            "Exact Match :  54.93827160493827\n",
            "F1 score    :  84.2159443858211\n",
            "--------------------------------------------------\n",
            "Label       : no_question_overlap\n",
            "N examples  :  672\n",
            "Exact Match :  60.56547619047619\n",
            "F1 score    :  86.71763270137613\n",
            "--------------------------------------------------\n",
            "Label       : answer_overlap\n",
            "N examples  :  2297\n",
            "Exact Match :  52.54680017414018\n",
            "F1 score    :  83.78062285910569\n",
            "--------------------------------------------------\n",
            "Label       : no_answer_overlap\n",
            "N examples  :  1313\n",
            "Exact Match :  67.0982482863671\n",
            "F1 score    :  89.17030980546777\n",
            "--------------------------------------------------\n",
            "Label       : answer_overlap_only\n",
            "N examples  :  315\n",
            "Exact Match :  53.01587301587302\n",
            "F1 score    :  84.71976179394723\n",
            "\n",
            "\n",
            "Évaluation pour le dataset: triviaqa\n",
            "Erreur lors de l'évaluation de la prédiction : \"['\"\"He had previously always said that each had been \"\"\"\"The best Olympics ever\"\"\"\"\"\"']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['\"\"Michael Jackson\\'s \"\"\"\"Thriller\"\"\"\"\"\"', \"\"Michael Jackson's Thriller\"\", 'Michael jackson thriller', \"\"Michael Jackson's Thriller (disambiguation)\"\", 'Michael Jacksons Thriller']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['\"\"\"\"\"\"Dictes or Sayenges of Phylosophers\"\"\"\"\"\"']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['\"\"\"\"\"\"Episode IV: A New Hope\"\"\"\" & \"\"\"\"Episode V: The Empire Strikes Back\"\"\"\"\"\"']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['\"\"\"\"\"\"The Cider House Rules\"\"\"\" & \"\"\"\"Hannah and Her Sisters\"\"\"\"\"\"']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['\"\"Winning the TV show \"\"\"\"American Idol\"\"\"\"\"\"']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['Here be dragons', 'Hic sunt leones', 'Hic Abundant Leones', 'Here there be Dragons', 'Here by dragons', 'Hic abundant leones', 'Here There Be Tigers', 'Here are lions', 'Here Be dragons', 'Here There be Dragons', 'HIC SVNT DRACONES', 'HC SVNT DRACONES', 'Hic Sunt Dracones', 'Here be Dragons', 'Here there be Tigers', '\"\"\"\"\"\"here be dragons\"\"\"\"\"\"', 'Hic abundant Leones', 'HC SVHT DRACONES', 'Hic sunt Dracones', 'Hic sunt dracones', 'Thar be dragons']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['\"\"John Cage after the premiere of 4\\'33\"\"\"\"\"\"']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['Waribashi', 'Chop-Sticks', 'Kuàizi', \"\"Kuài'er\"\", 'Hashi', 'ChopStick', 'Saibasi', 'Ryoribashi', 'Chop-stick', '\"\"\"\"\"\"Chopsticks\"\"\"\"\"\"', 'My hashi', 'Doi Dua', 'Zhù', 'Chop-Stick', 'ChopSticks', \"\"K'uai-erh\"\", 'Chopstix', \"\"K'uai-tzu\"\", 'Chop sticks', 'Chopsticks', 'Chop Stick', 'Chopstiks', \"\"Kuai'er\"\", 'Saibashi', 'Chop-sticks', 'Chop Sticks', 'Chopstick', 'Kuaizi', 'Japanese kitchen chopsticks', 'Chopstics', 'Chop stick', '筷子']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['Turn Turn Turn', 'Turn! Turn! Turn! (song)', 'Turn turn turn', 'Turn! Turn! Turn! (To Everything There Is A Season)', 'Turn! Turn! Turn!', '\"\"\"\"\"\"Turn! Turn! Turn! (to Everything There is a Season)\"\"\"\"\"\"', 'To Everything There is a Season', 'Turn! Turn! Turn! (To Everything There Is a Season)', 'To Everything There Is A Season', 'Turn! Turn! Turn! (to Everything There Is a Season)']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['Zadok The Priest', 'Zadok the Priest', '\"\"\"\"\"\"Zadok the Priest\"\"\"\"\"\"', 'Zadoc the Priest', 'Zadok the priest']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['Like a Prayer', 'Like A Prayer', 'Like a Prayer (disambiguation)', '\"\"\"\"\"\"Like a Prayer\"\"\"\"\"\"']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['Fringillidae', '\"\"\"\"\"\"Finch\"\"\"\"\"\"', 'Carduelidae', 'Carduelini', 'Finches', 'True finch', 'Finch', 'Finch (bird)']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['Franklin D. Roosevelt 1933 presidential inauguration', '1st inauguration of Franklin D. Roosevelt', '\"\"\"\"\"\"The only thing we have to fear is FEAR ITSELF\"\"\"\"\"\"', 'Only thing we have to fear is fear itself', 'The only thing we have to fear is fear itself', 'First inauguration of Franklin D. Roosevelt']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "Erreur lors de l'évaluation de la prédiction : \"['\"\"\"\"\"\"SOUND AND LIGHT\"\"\"\"\"\"']\"\n",
            "Détails de l'erreur : unterminated triple-quoted string literal (detected at line 1) (<unknown>, line 1)\n",
            "--------------------------------------------------\n",
            "Label       : total\n",
            "N examples  :  11313\n",
            "Exact Match :  11.959692389286662\n",
            "F1 score    :  40.89050995036883\n",
            "--------------------------------------------------\n",
            "Label       : question_overlap\n",
            "N examples  :  336\n",
            "Exact Match :  3.2738095238095237\n",
            "F1 score    :  34.8384028555621\n",
            "--------------------------------------------------\n",
            "Label       : no_question_overlap\n",
            "N examples  :  665\n",
            "Exact Match :  12.481203007518797\n",
            "F1 score    :  41.34153176278063\n",
            "--------------------------------------------------\n",
            "Label       : answer_overlap\n",
            "N examples  :  8112\n",
            "Exact Match :  1.9230769230769231\n",
            "F1 score    :  32.10273172417805\n",
            "--------------------------------------------------\n",
            "Label       : no_answer_overlap\n",
            "N examples  :  3201\n",
            "Exact Match :  37.39456419868791\n",
            "F1 score    :  63.16056836051043\n",
            "--------------------------------------------------\n",
            "Label       : answer_overlap_only\n",
            "N examples  :  411\n",
            "Exact Match :  0.7299270072992701\n",
            "F1 score    :  29.638667135139922\n",
            "\n",
            "\n",
            "Évaluation pour le dataset: webquestions\n",
            "--------------------------------------------------\n",
            "Label       : total\n",
            "N examples  :  2032\n",
            "Exact Match :  66.33858267716535\n",
            "F1 score    :  83.01919564976794\n",
            "--------------------------------------------------\n",
            "Label       : question_overlap\n",
            "N examples  :  274\n",
            "Exact Match :  56.2043795620438\n",
            "F1 score    :  77.76521804239765\n",
            "--------------------------------------------------\n",
            "Label       : no_question_overlap\n",
            "N examples  :  724\n",
            "Exact Match :  71.13259668508287\n",
            "F1 score    :  85.17741947041472\n",
            "--------------------------------------------------\n",
            "Label       : answer_overlap\n",
            "N examples  :  1176\n",
            "Exact Match :  58.24829931972789\n",
            "F1 score    :  79.58675470031656\n",
            "--------------------------------------------------\n",
            "Label       : no_answer_overlap\n",
            "N examples  :  856\n",
            "Exact Match :  77.45327102803738\n",
            "F1 score    :  87.73479209434076\n",
            "--------------------------------------------------\n",
            "Label       : answer_overlap_only\n",
            "N examples  :  299\n",
            "Exact Match :  62.541806020066886\n",
            "F1 score    :  81.75393188225894\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WTRXnfNxv30o"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}